---
title: "BN - Assignment 2"
author: "Thomas Rost"
date: "Saturday, December 13, 2014"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
    toc: yes
  html_document:
    toc: yes
lof: yes
bibliography: C:\Users\Trost\Copy\Nijmegen\Storage\Docs\Thomas\Mendeley_Bib\library.bib
---



\newpage

<!---


\section{test}



First Header  | Second Header
------------- | -------------
Content Cell  | Content Cell
Content Cell  | Content Cell


[-@Steele1999]
@Steele1999
-->

#Already there:

## Investigate what is the effect of data discretisation on the structure of the learnt Bayesian
network, e.g., in terms of number of arcs, changes in arc direction, etc., by considering
three discretisations of the continuous features in the iris dataset with different number
of bins (see Section B.3). Do this for both classes of learning algorithms and compare
the results.

###there: get discretization for data set

###Missing: write what "number of arcs, changes in arc direction, etc" changes there are

##Investigate what is the effect of the size of the dataset on the structure of the learnt
Bayesian network by considering three subsets of the breast cancer dataset with different
7number of samples. Do this for both classes of learning algorithms and compare the
results.

###there: get size of dataset changed and plot

###Missing: invesitgate "number of arcs, changes in arc direction, etc"

##Define measures that can be used to determine the quality of a learning algorithm in
terms of a known Bayesian network structure. Motivate the definition of these measures


###there: Define ???

###Missing: motivate???

## Use these measures in order to evaluate the quality of both classes of learning algorithms
for the NHL dataset.

###there: plot NHL algorithm

###Missing: evaluate NHL with new measure

##Compare the marginal probability distributions of the learnt NHL Bayesian network with
those of the manually constructed network.

###there: marginal probabilities

###Missing: comparisson, find manually constructed network

## Learn a Bayesian network from the breast cancer dataset using a search-and-score or
constraint-based algorithm

###there: Done

###Missing:

##Develop a special purpose (e.g., NBN or TAN) Bayesian network (see Section B.2 in
the appendix).

###there: Done

###Missing:

##Compare the Bayesian network obtained by step (6) and the special purpose Bayesian
network from (7) with the manually constructed network in Figure 2 in terms of network
structure, goodness of fit scores (e.g., likelihood) and accuracy using measures such as
misclassification error and area under the ROC curve from cross-validation (see Section
B.2 in the appendix)

###there:

###Missing: comparission


# comparison of two learning algos

## data discretization
data discretization: number of arcs, changes in arc direction,...
three discretizations in the iris data with different number of bins (B.3). Do this for both classes (constraing based oder search and score). Comppare the results




```{r, echo=FALSE, include=FALSE}
library(datasets)
library(Rgraphviz)
library(gRain)
library(bnlearn)
library(caTools)


```

```{r,fig.cap=c("SB , ibreaks = 3","CB , ibreaks = 3","SB , ibreaks = 5", "CB , ibreaks = 5", "SB , ibreaks = 7","CB , ibreaks = 7")}


for( bins in c(3,5,7)){
tmp=discretize(iris[-5], method = 'hartemink',ibreaks=bins) 
NewIris = cbind(tmp,iris[5]) 

IrisNetsb <- tabu(NewIris)

plot(IrisNetsb, font.main = 1, main = paste("SB, ibreaks = ", bins))

}


for( bins in c(3,5,7)){
tmp=discretize(iris[-5], method = 'hartemink',ibreaks=bins) 
NewIris = cbind(tmp,iris[5]) 

IrisNetcb <- iamb(NewIris)

plot(IrisNetcb, font.main = 1, main = paste("CB, ibreaks = ", bins))

}




```

\clearpage






constraint based:
\begin{itemize}
\item grow-shrink (GS())
\item incremental association markov blanket (IAMB())
\item Fast incremental association (FAST-IAMB())
\item interleaved incremental Association (inter-IAMB)
\end{itemize}

score based learning algos:
\begin{itemize}
\item Hill climbing (HC)
\item Tabu search (Tabu)
\end{itemize}

\newpage

## size of dataset
consider subsets of BC-dataset with different number of samples. both classes of learning algios, compare results


```{r}

a <- read.csv('bc.csv')

for(part in c(10,2,1)){
    ind = sample(1:(nrow(a)/part))
    BC <- a[ind,]
    
    BC_cbl <- tabu(BC)
    
    plot(BC_cbl , font.main = 1, main = paste("CB, every ", part, "Datapoint"))
}


for(part in c(1,2,10)){
    ind = sample(1:(nrow(a)/part))
    BC <- a[ind,]
    
    BC_sbl <- iamb(BC)

    plot(BC_sbl,  font.main = 1, main = paste("SB, every ", part, "Datapoint"))
}

```


\clearpage


## comparison to manually constructed bayes network

```{r,include=FALSE}
library(Hmisc)

```

```{r}


Tmp = read.csv("nhl.csv",nrows = 5)


class = rep(list("factor"),ncol(Tmp))


NHL = read.csv("nhl.csv",colClasses = class)


#impute(NHL[,c(2:ncol(Tmp))], fun=median)
#impute(NHL)
#impute(NHL[c(2:3),],fun=median)

Tmp2<- complete.cases(NHL)
NHL <- NHL[Tmp2,]


NHLnet_sbl <- tabu(NHL)
NHLnet_cbl <- iamb(NHL)

plot(NHLnet_sbl)
title("NHL sbl")
plot(NHLnet_cbl)
title("NHL cbl")
```

NHL dataset, liad column classes as factor (A.4)

missing data : probably median since mean desn't make sense for factors.

check for differences, unterschiedlcihe zahl nodes is ok.

#define measures for quality of a learning algorithm in terms of a known bayes network structure, motivate definition

I have to be hones: I do not really understand that Question. Do the authors suggest I find a measure that is original in describing the quality of a bayesian Network? I'm sure that can't be true.

My measure for quality of the learning algorithm is Aruhga, which is the Quotient between nodes and vertices. $$Aruhga = \frac{Nodes}{Vertices}$$Arhuga is a binary measure with the classes {useful, not useful}, depending on wether Aruhga falls inside the interval [$\alpha , \beta$]. $\alpha$ and $\beta$ are depending heavily on the purpose and topic of the network and should normally be defined by logical reasoning before constructing the network. However, since the expert knowledge and necessary publications are not always readily available, the author provides some default values with $\alpha = 2.3$ and  $\beta = 1.2/Nodes!$. Please note that Aruhga is a measure of the connectivity of the network. values above alpha may indicate that too many nodes are used, thus either weakening the correlation between the single nodes OR bringing in Nodes that are not relevant for each other. Networks with a value below beta indicate that the network is highly connected which might reduce functionality and also might point to a circular relationship between variables in the real world that cannot be represented by a DAG.

##use measures to evaluate both classes of learning algs for NHL dataset

### iris data set
learnt with score learning algorithm: Aruhga = 5/4 = 1.25; beta = 1.2/5 = 0.002.
learnt with Constraint based algorithm: Aruhga = 5/2 = 2.5; beta = 1.2/5 = ,002

While the score learning algorithm scores well in terms of Aruhga, the constraint based algorithm falls out of the default interval. Given that sepal.length, sepal.width and petal.length are likely caused by the species, we would say that the constraint based algorithm performs poorly on this dataset. 

###Breast cancer dataset

nodes = 16,
vertices = scoreFull = 

### NHL

## compare margial prob distributions of learnt NHL bN with those of the manually coinstructed ones

since constraint based aslgorithms do not necessarily produce directed graphs, we will only examine score based algorithms.

```{r}
fittedNHL = bn.fit(NHLnet_sbl,NHL)

#fittedNHL2 = bn.fit(NHLnet_cbl,NHL)
# diagnostic BN structures

fittedNHL
#fittedNHL2
```


get the manually constructed network:


```{r}

manual = read.net("nhl.net")

#manual

#manNet = bn.fit(manual,a)
names(manual)
typeof(manual)
manual
test <- bn.net(manual)
plot(test)
```

# develop special purpose BN (NBN or TAN, see .2)
```{r}


netnb = naive.bayes(a,"BC")
plot(netnb)
netnb2 <- bn.fit(netnb,a)
#netnb2
#score(netnb,a)
#score(netnb2)
#score(a,BC_cbl)



```



## compare BN and SPBN with manually constructed network in terms of network structure, goodnes of fit scores (lkikelohood,...) and accuracy measures such as misclassification error and ROC (B.2)



```{r}

netnb2 <- bn.fit(netnb,a)
netnb2
score(netnb,a)


nbncv <- bn.cv(a,netnb,loss="logl",loss.args = list(target="BC"),debug=T)
nbncv

bncv <- bn.cv(a,BC_cbl,loss="logl",loss.args = list(target="BC"),debug=T)
bncv


nbncv <- bn.cv(a,netnb,loss="pred",loss.args = list(target="BC"),debug=T)
nbncv

bncv <- bn.cv(a,BC_cbl,loss="pred",loss.args = list(target="BC"),debug=T)
bncv




```

Read in manually constructed network:

```{r}

manual = read.net("bc.net")

#manual

#manNet = bn.fit(manual,a)
names(manual)
names(BC_cbl)
typeof(manual)
typeof(BC_cbl)
test <- bn.net(manual)
plot(test)


bncv <- bn.cv(a,test,loss="pred",loss.args = list(target="BC"),debug=T)

#manbncv <- bn.cv(a,manual,loss="logl",loss.args = list(target="BC"),debug=T)
#manbncv


```




library(ROCR),
\footnote{Tobias Sing, Oliver Sander, Niko Beerenwinkel, Thomas Lengauer.
ROCR: visualizing classifier performance in R.
Bioinformatics 21(20):3940-3941 (2005).}
\cite{Sing2005}
@Sing2005
 







<!---
# Ideas:


```{r}
library(Rgraphviz)
library(gRain)
library(bnlearn)
library(caTools)
data(asia)

net = hc(asia)
plot(net)
net <- set.arc(net,"A","T")

plot(net)
fitted = bn.fit(net,asia)
fitted

score(net,asia)

netgs = gs(asia)
netgs_directed = cextend(netgs)
netgs_directed
plot(netgs)
plot(netgs_directed)


netnb = naive.bayes(asia,"T",names(asia)[c(1:2,4:8)])
plot(netnb)
```

Do Cross validation!

```{r}
netcv = bn.cv(asia,net,loss ="pred", k = 3, loss.args = list(target="T"),debug = T)
```
\newpage
-->
#Attachment A

## Iris dataset

### other discretizations:


```{r}
NewIris <-  discretize(iris, method = 'interval', breaks = 3, ibreaks = 3)
NewIris1 <-  discretize(iris, method = 'interval', breaks = 100, ibreaks = 10)

NewIrisNetHc <- tabu(NewIris)
NewIrisNetGs <- iamb(NewIris)

NewIrisNetHc
NewIrisNetGs

plot(NewIrisNetHc)
plot(NewIrisNetGs)

NewIrisNetHc1 <- tabu(NewIris)
NewIrisNetGs1 <- iamb(NewIris)

NewIrisNetHc1
NewIrisNetGs1

```


### Values of hartemink

```{r}
for( bins in c(3,5,7)){
tmp=discretize(iris[-5], method = 'hartemink',ibreaks=bins) 
NewIris = cbind(tmp,iris[5]) 

IrisNetsb <- tabu(NewIris)

print(IrisNetsb)
}


for( bins in c(3,5,7)){
tmp=discretize(iris[-5], method = 'hartemink',ibreaks=bins) 
NewIris = cbind(tmp,iris[5]) 

IrisNetcb <- iamb(NewIris)

print(IrisNetcb)
}
```


##BC Dataset

### values depending on size

```{r}
for(part in c(1,2,10)){
    ind = sample(1:(nrow(a)/part))
    BC <- a[ind,]
    
    BC_cbl <- tabu(BC)
    
    print(BC_cbl)
}


for(part in c(1,2,10)){
    ind = sample(1:(nrow(a)/part))
    BC <- a[ind,]
    
    BC_sbl <- iamb(BC)
    print(BC_sbl)

}
```



##NHL dataset

###Values of the networks

INSERTXXX

\newpage

#list of figures

\listoffigures
\newpage

#references
\twocolumn